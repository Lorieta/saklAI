"""
Comprehensive Metrics Validation - Check All Available Metrics
"""

import json
import sys
import os
from datetime import datetime

# Add the evaluation directory to path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from quantitative_metrics import QuantitativeMetrics, BusinessMetrics, QuantitativeAnalyzer, generate_quantitative_report
from sample_business_data import generate_sample_business_data

def check_all_metrics():
    """Comprehensive check of all available metrics in the SaklAI evaluation system."""
    
    print("üîç COMPREHENSIVE METRICS VALIDATION")
    print("=" * 60)
    
    # 1. CHECK CORE DATACLASS METRICS
    print("\nüìä 1. CORE QUANTITATIVE METRICS (QuantitativeMetrics Dataclass)")
    print("-" * 50)
    
    metrics = QuantitativeMetrics()
    core_metrics = [
        # Response Quality Metrics
        ("bleu_score", "BLEU Score", "Translation/Generation Quality"),
        ("rouge_1_score", "ROUGE-1", "Unigram Overlap"),
        ("rouge_2_score", "ROUGE-2", "Bigram Overlap"), 
        ("rouge_l_score", "ROUGE-L", "Longest Common Subsequence"),
        ("semantic_similarity_score", "Semantic Similarity", "Meaning Preservation"),
        ("lexical_diversity", "Lexical Diversity", "Vocabulary Richness"),
        
        # Performance Metrics
        ("avg_response_time", "Average Response Time", "Mean Latency"),
        ("median_response_time", "Median Response Time", "Median Latency"),
        ("p95_response_time", "95th Percentile", "P95 Latency"),
        ("p99_response_time", "99th Percentile", "P99 Latency"),
        ("throughput_qps", "Throughput QPS", "Queries Per Second"),
        
        # Accuracy Metrics
        ("exact_match_accuracy", "Exact Match", "Perfect Matches"),
        ("fuzzy_match_accuracy", "Fuzzy Match", "Approximate Matches"),
        ("classification_precision", "Classification Precision", "Positive Predictive Value"),
        ("classification_recall", "Classification Recall", "Sensitivity"),
        ("classification_f1", "Classification F1", "Harmonic Mean"),
        
        # RAG-Specific Metrics
        ("retrieval_precision_at_k", "Retrieval Precision@K", "Relevant Retrieved"),
        ("retrieval_recall_at_k", "Retrieval Recall@K", "Retrieved Relevant"),
        ("retrieval_mrr", "Mean Reciprocal Rank", "Ranking Quality"),
        ("context_relevance_score", "Context Relevance", "Context Quality"),
        ("answer_faithfulness", "Answer Faithfulness", "Factual Accuracy"),
        
        # Translation-Specific Metrics
        ("translation_adequacy", "Translation Adequacy", "Meaning Preservation"),
        ("translation_fluency", "Translation Fluency", "Naturalness"),
        ("character_error_rate", "Character Error Rate", "Character-level Errors"),
        ("word_error_rate", "Word Error Rate", "Word-level Errors"),
        
        # Statistical Metrics
        ("confidence_interval_lower", "CI Lower", "Confidence Interval Lower"),
        ("confidence_interval_upper", "CI Upper", "Confidence Interval Upper"),
        ("standard_deviation", "Standard Deviation", "Variability"),
        ("variance", "Variance", "Spread"),
        
        # System Metrics
        ("task_completion_rate", "Task Completion", "Success Rate"),
        ("user_satisfaction_proxy", "User Satisfaction", "Satisfaction Proxy"),
        ("error_rate", "Error Rate", "Failure Rate"),
        ("system_reliability", "System Reliability", "Uptime/Reliability")
    ]
    
    for attr, name, description in core_metrics:
        if hasattr(metrics, attr):
            value = getattr(metrics, attr)
            print(f"  ‚úÖ {name:<25} | {attr:<30} | {description}")
        else:
            print(f"  ‚ùå {name:<25} | {attr:<30} | Missing!")
    
    print(f"\nüìà Total Core Metrics: {len(core_metrics)}")
    
    # 2. CHECK BUSINESS METRICS CALCULATIONS
    print("\nüíº 2. BUSINESS METRICS (BusinessMetrics Class)")
    print("-" * 50)
    
    business_calc = BusinessMetrics()
    business_methods = [
        ("calculate_average_handoff_time", "Average Handoff Time", "Bot‚ÜíHuman Transition Time"),
        ("calculate_bot_containment_rate", "Bot Containment Rate", "Bot Resolution Success"),
        ("calculate_csat", "Customer Satisfaction", "CSAT Score & NPS"),
        ("calculate_cost_per_session", "Cost Analysis", "Session & Inference Costs"),
        ("calculate_first_contact_resolution", "First Contact Resolution", "FCR Rate & Efficiency")
    ]
    
    for method, name, description in business_methods:
        if hasattr(business_calc, method):
            print(f"  ‚úÖ {name:<25} | {method:<35} | {description}")
        else:
            print(f"  ‚ùå {name:<25} | {method:<35} | Missing!")
    
    print(f"\nüí∞ Total Business Metrics: {len(business_methods)}")
    
    # 3. CHECK QUANTITATIVE ANALYZER METHODS
    print("\nüî¨ 3. ANALYZER CALCULATION METHODS (QuantitativeAnalyzer Class)")
    print("-" * 50)
    
    analyzer = QuantitativeAnalyzer()
    analyzer_methods = [
        ("calculate_bleu_score", "BLEU Calculation", "N-gram Overlap"),
        ("calculate_rouge_scores", "ROUGE Calculation", "Summary Quality"),
        ("calculate_semantic_similarity", "Semantic Analysis", "Embedding Similarity"),
        ("calculate_lexical_diversity", "Lexical Analysis", "Vocabulary Richness"),
        ("calculate_performance_metrics", "Performance Analysis", "Latency Statistics"),
        ("calculate_classification_metrics", "Classification Analysis", "Precision/Recall/F1"),
        ("calculate_retrieval_metrics", "Retrieval Analysis", "Information Retrieval"),
        ("calculate_translation_metrics", "Translation Analysis", "Translation Quality"),
        ("calculate_confidence_interval", "Statistical Analysis", "Confidence Intervals"),
        ("analyze_evaluation_results", "Main Evaluation", "Complete Analysis")
    ]
    
    for method, name, description in analyzer_methods:
        if hasattr(analyzer, method):
            print(f"  ‚úÖ {name:<25} | {method:<35} | {description}")
        else:
            print(f"  ‚ùå {name:<25} | {method:<35} | Missing!")
    
    print(f"\nüßÆ Total Analyzer Methods: {len(analyzer_methods)}")
    
    # 4. TEST BUSINESS METRICS WITH SAMPLE DATA
    print("\nüß™ 4. TESTING BUSINESS METRICS WITH SAMPLE DATA")
    print("-" * 50)
    
    # Generate sample business data
    business_data = generate_sample_business_data()
    
    # Test each business metric
    try:
        # 1. Average Handoff Time
        handoff_result = business_calc.calculate_average_handoff_time(business_data["handoff_events"])
        if "error" not in handoff_result:
            print(f"  ‚úÖ Handoff Time: {handoff_result.get('average_handoff_time', 0):.2f}s (median: {handoff_result.get('median_handoff_time', 0):.2f}s)")
        else:
            print(f"  ‚ö†Ô∏è Handoff Time: {handoff_result.get('error', 'No data')}")
        
        # 2. Bot Containment Rate
        containment_result = business_calc.calculate_bot_containment_rate(business_data["sessions"])
        print(f"  ‚úÖ Bot Containment: {containment_result.get('bot_containment_rate', 0):.1f}% ({containment_result.get('bot_resolved_count', 0)}/{containment_result.get('total_sessions', 0)} sessions)")
        
        # 3. CSAT
        csat_result = business_calc.calculate_csat(business_data["satisfaction_data"])
        print(f"  ‚úÖ CSAT Score: {csat_result.get('csat_score', 0):.1f}% (avg rating: {csat_result.get('average_rating', 0):.1f}/5.0)")
        
        # 4. Cost Analysis
        cost_result = business_calc.calculate_cost_per_session(business_data["usage_data"])
        print(f"  ‚úÖ Cost Analysis: ${cost_result.get('cost_per_session', 0):.3f}/session, ${cost_result.get('cost_per_inference', 0):.4f}/inference")
        
        # 5. First Contact Resolution
        fcr_result = business_calc.calculate_first_contact_resolution(business_data["resolution_data"])
        print(f"  ‚úÖ FCR Rate: {fcr_result.get('fcr_rate', 0):.1f}% ({fcr_result.get('first_contact_resolutions', 0)}/{fcr_result.get('total_contacts', 0)} contacts)")
        
    except Exception as e:
        print(f"  ‚ùå Error testing business metrics: {e}")
    
    # 5. TEST COMPLETE EVALUATION PIPELINE
    print("\nüîÑ 5. TESTING COMPLETE EVALUATION PIPELINE")
    print("-" * 50)
    
    # Create mock evaluation data
    eval_data = [
        {
            "question": "What is my account balance?",
            "expected": "Your current account balance is $1,250.50",
            "actual": "Your account balance is $1,250.50",
            "response_time": 2.3,
            "success": True
        },
        {
            "question": "How do I transfer money?",
            "expected": "To transfer money, log into online banking and select Transfer Funds",
            "actual": "You can transfer money through online banking by selecting Transfer Funds",
            "response_time": 1.8,
            "success": True
        },
        {
            "question": "What are your loan rates?",
            "expected": "Our current loan rates range from 3.5% to 7.2% depending on the loan type",
            "actual": "Current loan rates are 3.5% to 7.2% based on loan type and credit score",
            "response_time": 3.1,
            "success": True
        }
    ]
    
    try:
        # Run complete analysis
        complete_metrics = analyzer.analyze_evaluation_results(eval_data)
        
        print(f"  ‚úÖ Complete Analysis: Success")
        print(f"     ‚Ä¢ BLEU Score: {complete_metrics.bleu_score:.3f}")
        print(f"     ‚Ä¢ ROUGE-L: {complete_metrics.rouge_l_score:.3f}")
        print(f"     ‚Ä¢ Avg Response Time: {complete_metrics.avg_response_time:.2f}s")
        print(f"     ‚Ä¢ Success Rate: {complete_metrics.task_completion_rate:.1%}")
        print(f"     ‚Ä¢ System Reliability: {complete_metrics.system_reliability:.1%}")
        
        # Generate comprehensive report
        report = generate_quantitative_report(complete_metrics, business_data)
        print(f"  ‚úÖ Report Generation: Success ({len(report.split(chr(10)))} lines)")
        
    except Exception as e:
        print(f"  ‚ùå Error in complete pipeline: {e}")
    
    # 6. SUMMARY
    print(f"\nüìã 6. METRICS SUMMARY")
    print("=" * 60)
    
    total_core = len(core_metrics)
    total_business = len(business_methods)
    total_analyzer = len(analyzer_methods)
    total_metrics = total_core + total_business + total_analyzer
    
    print(f"üìä Core Quantitative Metrics: {total_core}")
    print(f"üíº Business Intelligence Metrics: {total_business}")
    print(f"üî¨ Analyzer Calculation Methods: {total_analyzer}")
    print(f"üéØ TOTAL AVAILABLE METRICS: {total_metrics}")
    
    print(f"\nüéâ ALL METRICS VALIDATION COMPLETE!")
    print(f"   System Status: Enterprise-Ready ‚úÖ")
    print(f"   Business Intelligence: Fully Operational ‚úÖ")
    print(f"   Technical Evaluation: Comprehensive ‚úÖ")
    
    return {
        "core_metrics": total_core,
        "business_metrics": total_business,
        "analyzer_methods": total_analyzer,
        "total_metrics": total_metrics,
        "status": "operational"
    }

if __name__ == "__main__":
    results = check_all_metrics()
    print(f"\nüìà Validation Results: {results}")
